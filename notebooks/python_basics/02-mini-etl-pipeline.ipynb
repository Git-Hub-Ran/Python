{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs/X2F10OuRtnXtslMSYrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Git-Hub-Ran/Python/blob/main/notebooks/python_basics/02-mini-etl-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: left\">\n",
        "\n",
        "# üêç Mini ETL Pipeline in Python\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Task Description\n",
        "\n",
        "**Goal:** Implement a mini **ETL pipeline** using Python and pandas.\n",
        "\n",
        "### Steps:\n",
        "1. üìÇ **Load a raw data file**  \n",
        "   e.g., a CSV of sales or sensor data.\n",
        "2. üßπ **Clean / Transform the data**  \n",
        "   - Filter, aggregate, or modify columns as needed.\n",
        "3. üìä **Output a summary or visualization**  \n",
        "   - Aggregate totals per category  \n",
        "   - Plot the results\n",
        "\n",
        "This project helps you **solidify Python skills** and demonstrates your ability to apply **data engineering principles** in code.\n",
        "\n"
      ],
      "metadata": {
        "id": "lBn5zHinvOZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Read a csv file with Sales data:\n",
        "url = \"https://gist.githubusercontent.com/denandreychuk/b9aa812f10e4b60368cff69c6384a210/raw/100%20Sales%20Records.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "#Chacking if 'Order ID' can be an index:\n",
        "print(df['Order ID'].is_unique)"
      ],
      "metadata": {
        "id": "l70IPwojvgqd",
        "outputId": "9a0c6b65-a6f6-4db5-f707-f77fb2fb1346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define 'Order ID' as Index:\n",
        "df.set_index('Order ID')\n",
        "\n",
        "#Shows amount of orders per country:\n",
        "grouped=df.groupby('Country').Country.count().rename('Count')\n",
        "\n",
        "#See the country with the highest amount of orders on top:\n",
        "grouped.sort_values(ascending=False)\n",
        "\n",
        "#show the total amount of countries:\n",
        "len(grouped.index)\n",
        "\n",
        "#Plot the results:\n"
      ],
      "metadata": {
        "id": "_tcQVpQt1y5O",
        "outputId": "520ba698-d066-4379-b44b-17d141369f81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}